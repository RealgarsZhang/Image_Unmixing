This is a program solving the image unmixing problem with machine learning.

The structure of the files:

    The "examples" folder:
        Here are the examples generated by the matlab code. Attention: the readinExamples() function( in NoNoiseNoPrintCPU.c or NoNoiseNoPrintMagma.c) will read in the training, CV and test examples. If you put the examples in directory “myFolder”, please change the corresponding directory name in the function. ATTENTION! You should also specify the number of training examples at the beginning of mainfCPU.c or mainfMagma.c Refer to the matlab code of this project for the details.

    The following are the main files of the program: 
    First set: The CPU code using LAPACK and BLAS:
               NoNoiseNoPrintCPU.h
               NoNoiseNoPrintCPU.c(all important functions are here.)
               mainfCPU.c ( main() is called here.)
    Compilation :make clean.Then type "make CPUall" then everything compiles. 


    Secdon set: The GPU code using MAGMA:
                NoNoiseNoPrintMagma.h
                NoNoiseNoPrintMagma.c
                mainfMagma.c
    Compilation: Set the directory of MAGMA and cuda/8.0 library and include in the Makefile. make clean.Then type "make all”.

The structure of the program:
    In the CPU code, in the costFunction(), I use for loop quite often. cblas_dgemm() is called a lot.  The Theta[] and Theta_grad[] are the arrays of MATRIX struct where the pointers point to corresponding positions nn_params and grad, which are the NN parameters and the gradient.

    In the GPU code, to accelerate, d_delta[],d_Theta_grad[],d_Theta[] and d_a[] are all initialized outside costFunction(). All the way through this program, d_Theta[] are pointing to positions in d_nn_params and d_Theta_grad[] are pointing to positions in d_grad. This is very important - no other variables will take the value of NN parameters and gradient. This is why in the fmincg(),I fist callthe costFunction() and then copy d_grad to d_df1 or d_df2. Regularization in this GPU code is also  for all of the NN parameters, to accelerate.  

Technical details:
     1. Send the training examples into the ./examples directory. Then change
the TRAINING_SIZE, CV_SIZE and TEST_SIZE in main() correspondingly.
     2. Gradient check code is in the main().  
     3. dla_pmult_dtanh.cu and dlatanh.cu are two cuda files defining the       function to apply (elementwise multiplication and derivative of tanh) and (
elementwise tanh) on GPU. Check the content in Makefile to find out how to compile them. 
